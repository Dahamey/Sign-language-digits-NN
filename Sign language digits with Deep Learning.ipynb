{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7982018,"sourceType":"datasetVersion","datasetId":4698109}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* * ****<font color='green'>\n**Welcome to Deep Learning Tutorial for Beginners** \n* I am going to explain <u>every thing</u> one by one.\n* Instead of writing long and hard for reading paragraphs, I define and emphasize keywords line by line.\n* At the end of this tutorial, you will have enough information about deep learning to go deeper inside it.\n* Lets look at content.\n\n<font color='red'>\n<br>Content:\n    \n* [Introduction](#1)\n* [Overview the Data Set](#2)\n* [Logistic Regression](#3)\n    * [Computation Graph](#4)\n    * [Initializing parameters](#5)\n    * [Forward Propagation](#6)\n        * Sigmoid Function\n        * Loss(error) Function\n        * Cost Function\n    * [Optimization Algorithm with Gradient Descent](#7)\n        * Backward Propagation\n        * Updating parameters\n    * [Logistic Regression with Sklearn](#8)\n    * [Summary and Questions in Minds](#9)\n    \n* [Artificial Neural Network](#10)\n    * [2-Layer Neural Network](#11)\n        * [Size of layers and initializing parameters weights and bias](#12)\n        * [Forward propagation](#13)\n        * [Loss function and Cost function](#14)\n        * [Backward propagation](#15)\n        * [Update Parameters](#16)\n        * [Prediction with learnt parameters weight and bias](#17)\n        * [Create Model](#18)\n    * [L-Layer Neural Network](#19)\n        * [Implementing with keras library](#22)\n* Time Series Prediction: https://www.kaggle.com/kanncaa1/time-series-prediction-with-eda-of-world-war-2\n* [Artificial Neural Network with Pytorch Library](#23)\n* [Convolutional Neural Network with Pytorch Library](#24)\n* [Recurrent Neural Network with Pytorch Library](#25)\n* [Conclusion](#20)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n\n# INTRODUCTION\n* **Deep learning:** One of the machine learning technique that learns features directly from data. \n\n* **Why deep learning:** When the amounth of data is increased, machine learning techniques are insufficient in terms of performance and deep learning gives better performance like accuracy.\n\n<a href=\"http://ibb.co/m2bxcc\"><img src=\"http://preview.ibb.co/d3CEOH/1.png\" alt=\"1\" border=\"0\"></a>\n* **What is amounth of big:** It is hard to answer but intuitively 1 million sample is enough to say \"big amounth of data\"\n* **Usage fields of deep learning:** Speech recognition, image classification, natural language procession (NLP) or recommendation systems\n* **What is the difference of deep learning from machine learning:** \n    * Machine learning covers deep learning. \n    * Features are given machine learning manually.\n    * On the other hand, deep learning learns features directly from data.\n    \n<a href=\"http://ibb.co/f8Epqx\"><img src=\"http://preview.ibb.co/hgpNAx/2.png\" alt=\"2\" border=\"0\"></a>\n\n<br>Lets look at our data.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\n# filter warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode('utf8'))\n# Any results you write to the current directory are saved as output.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"Overview the Data Set\"></a> <br>\n# Overview the Data Set\n* We will use \"Sign language digits dataset\" for this tutorial.\n* In this data there are 2062 sign language digits images.\n* As you know digits are from 0 to 9. Therefore there are 10 unique sign.\n* At the beginning of tutorial we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Note: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much. \n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1).","metadata":{}},{"cell_type":"code","source":"# load dataset\nX_1 = np.load('/kaggle/input/sign-language-digits-dataset/X.npy')\ny_1 = np.load('/kaggle/input/sign-language-digits-dataset/Y.npy')\n\nimg_size = 64\n\n# Look at some signs :\nplt.subplot(1,2,1)\nplt.imshow(X_1[210].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(X_1[900].reshape(img_size, img_size))\nplt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_1.shape) \nprint(y_1.shape)   # 10 possible outcomes ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images.","metadata":{}},{"cell_type":"code","source":"# Join a sequence of arrays of 0s and 1s along the rows axis\nX = np.concatenate((X_1[204:409], X_1[822:1028]), axis = 0) # from 0 to 204 are zero signs and from 205 to 411 are one signs \nz = np.zeros(205)\no = np.ones(206)\ny = np.concatenate((z,o), axis = 0).reshape(X.shape[0], 1)\n\nprint(\"X shape : \", X.shape)\nprint(\"y shape : \", y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The shape of the X is (411, 64, 64)\n    * 411 means that we have 411 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (411,1)\n    *  411 means that we have 411 labels (0 and 1) \n* Lets split X and Y into train and test sets.\n    * test_size = percentage of test size. test = 15% and train = 75%\n    * random_state = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always creates same train and test distribution because we have same random_state.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T12:49:04.629333Z","iopub.execute_input":"2024-03-30T12:49:04.629806Z","iopub.status.idle":"2024-03-30T12:49:04.936255Z","shell.execute_reply.started":"2024-03-30T12:49:04.629766Z","shell.execute_reply":"2024-03-30T12:49:04.934924Z"}}},{"cell_type":"code","source":"# Let's create X_train, y_train, X_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\nprint(f\"train : {number_of_train}\\ntest : {number_of_test}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).\n","metadata":{}},{"cell_type":"code","source":"X_train_flatten = X_train.reshape(number_of_train,-1)\nX_test_flatten = X_test.reshape(number_of_test, -1)\n\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As you can see, we have 349 images for training and each image has 4096 pixels in image train array.\n* Also, we have 62 images for testing and each image has 4096 pixels in image test array.\n* Then let's take transpose.","metadata":{}},{"cell_type":"code","source":"X_train = X_train_flatten.T   # shape(4096, 349)\nX_test = X_test_flatten.T      # shape(4096, 62)\ny_train = y_train.T            # shape(1,349)\ny_test = y_test.T              # shape(1,62)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='purple'>\nWhat we did up to this point:\n    \n* Choose our labels (classes) that are sign zero and sign one\n* Create and flatten train and test sets\n* Our final inputs(images) and outputs(labels or classes) look like this:\n<a href=\"http://ibb.co/bWMK7c\"><img src=\"http://image.ibb.co/fOqCSc/3.png\" alt=\"3\" border=\"0\"></a>","metadata":{"execution":{"iopub.status.busy":"2024-03-30T13:11:51.298033Z","iopub.execute_input":"2024-03-30T13:11:51.299084Z","iopub.status.idle":"2024-03-30T13:11:51.305874Z","shell.execute_reply.started":"2024-03-30T13:11:51.299043Z","shell.execute_reply":"2024-03-30T13:11:51.304418Z"}}},{"cell_type":"markdown","source":"\n**Remark : instead of 348, it's 349**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# Logistic Regression\n* When we talk about binary classification( 0 and 1 outputs) what comes to mind first is logistic regression.\n* However, one may ask what is logistic regression doing in a deep learning tutorial ?\n* The answer is that  logistic regression is actually a very simple neural network. \n* By the way neural network and deep learning are same thing. When we will see artificial neural network, I will explain in details the term \"deep\".\n* In order to understand logistic regression (simple deep learning) lets first learn computation graph.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n##  Computation Graph\n* Computation graphs are a nice way to think about mathematical expressions.\n* It is like visualization of  mathematical expressions.\n* For example we have $$c = \\sqrt{a^2 + b^2}$$\n* It's computational graph is this. As you can see we express math with graph.\n<a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/hWn6Lx/d.jpg\" alt=\"d\" border=\"0\"></a>","metadata":{}},{"cell_type":"markdown","source":"* Now lets look at computation graph of logistic regression\n<a href=\"http://ibb.co/c574qx\"><img src=\"http://preview.ibb.co/cxP63H/5.jpg\" alt=\"5\" border=\"0\"></a>\n    * Parameters are weight and bias.\n    * Weights: coefficients of each pixels\n    * Bias: intercept\n    * $z = (w^T)X + b  => z$ equals to (transpose of weights times input $X$) + bias \n    * In an other saying $=> z = b + px1*w1 + px2*w2 + ... + px4096*w4096$\n    * $y_{head} = sigmoid(z)$\n    * Sigmoid function crushes $z$ to make it between zero and one so that becomes a probability. You can see sigmoid function in computation graph.\n* Why we use sigmoid function?\n    * It gives probabilistic result\n    * It is derivative so we can use it in gradient descent algorithm (we will see it later).\n* Let's take an example:\n    * Let's say we find $z = 4$ and put $z$ into sigmoid function. The result($y_{head}$) is almost 0.9. It means that our classification result is 1 with 90% probability.\n* Now lets start with from beginning and examine each component of computation graph more detailed.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Initializing parameters\n* As you know, input is our images that has 4096 pixels( in each image in X_train).\n* Each pixel have its own weights.\n* The first step is multiplying each pixels with their own weights.\n* The question is that what is the initial value of weights?\n    * There are some techniques that I will explain at artificial neural network but for this time initial weights are 0.01.\n    * Okey, weights are 0.01 but what is the weight array shape? As you can see from computation graph of logistic regression, it is (4096,1)\n    * Also initial bias is 0.\n* Let's write some code. In order to use it in coming topics like artificial neural network (ANN), I made some definitions (methods).","metadata":{}},{"cell_type":"code","source":"# let's initialize parameters\n# So what we need is dimension 4096 that is number \n# of pixels as a parameter for our initialize method(def)\n\ndef initialize_weights_and_bias(dimension) :\n    w = np.full((dimension,1), 0.01) # shape(dimension,1)\n    b = 0.0\n    return w, b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w,b = initialize_weights_and_bias(4096) # w is shape(4096,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w)\nprint(b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(w.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## Forward Propagation\n\n* All the steps from pixels to cost is called **forward propagation**\n    * $z = (w^T)X + b =>$ in this equation we know that $X$ is pixel array, $w$ (weights), $b$ (bias), and $T$ is transpose.\n    * Then we put $z$ into sigmoid function that returns $y_{head}$ (probability), (look at computation graph).\n    * Then we calculate loss(error) function. \n    * Cost function is summation of all loss(error).\n    * Let's start with $z$ and then write sigmoid definition(method) that takes $z$ as input parameter and returns $y_{head}$ (probability).","metadata":{}},{"cell_type":"code","source":"# Calculation of z\ndef sigmoid(z) :\n    y_head = 1/(1+ np.exp(-z))\n    return y_head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example :\nprint(sigmoid(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As we write sigmoid method and calculate $y_{head}$. Let's learn what is a loss(error) function\n* Let's take an example, I've put one image as input then multiply it with their weights and add bias term to have $z$. Then put $z$ into sigmoid method to find $y_{head}$. Up to this point we know what we did. Then e.g $y_{head}$ became 0.9 that is bigger than 0.5 so our prediction of the image is a sign \"one\" image. Ok, every thing looks fine. But, is our prediction correct ? and how do we check whether it is correct or not? The answer is we do it using the  **loss(error) function**:\n    * Mathematical expression of log loss(error) function is that: \n    <a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/eC0JCK/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"></a>\n    * It says that if you make wrong prediction, loss(error) becomes big. **DENKLEM DUZELTME**\n        * Example: Our real image is labeled as \"sign one\" with the label $y = 1$. Upon making a prediction, let's say $y_{\\text{head}} = 1$. When we substitute $y$ and $y_{\\text{head}}$ into the loss (error) equation, the result is 0. This indicates a correct prediction, hence our loss is 0. However, if we were to make an incorrect prediction, such as $y_{\\text{head}} = 0$, the loss (error) becomes infinity.\n\n* Following this, the cost function is the summation of the loss function. Each image contributes to the loss function. Therefore, the cost function is the summation of the loss functions generated by each input image.\n\n* Now, let's proceed to implement forward propagation.\n","metadata":{}},{"cell_type":"code","source":"# Forward propagation steps :\n# 1/ Find z = w^T * X Â° b\n# 2/ y_head = sigmoid(z)\n# 3/ loss(error) = loss(y, y_head)\n# 4/ cost = sum(loss)\n\ndef forward_propagation(w, b, X_train, y_train):\n    z = w.T @ X_train + b   # shape(1,349)\n    y_head = sigmoid(z)    # shape(1,349)\n    loss = -(1-y_train) * np.log(1-y_head) - y_train*np.log(y_head)  # shape(1,349)\n    cost = loss.sum() / X_train.shape[1]   # for scaling\n    return cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n##  Optimization Algorithm with Gradient Descent\n\n* Now that we understand our cost, which is the error, therefore, we need to decrease the cost because a high cost indicates that we made the wrong prediction.\n\n* Let's consider the **first step**: everything starts with initializing weights and biases. Therefore, the cost depends on them.\n\n* To decrease the cost, we need to update the weights and biases, in other words, our model needs to learn the parameters (weights and biases) that minimize the cost function. This technique is called **gradient descent**.\n\n* Let's illustrate this with an example:\n\nSuppose we have $w = 5$ and bias $= 0$ (ignore bias for now). Then, after forward propagation, our cost function is $1.5$.\nIt looks like this: (red lines)\n\n<a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/dAaYJH/7.jpg\" alt=\"7\" border=\"0\"></a>\n\n* As seen from the graph, we are not at the minimum point of the cost function. Therefore, we need to move towards the minimum cost. Ok, let's update the weight. (The symbol $:=$ denotes updating)\n* $w := w - \\text{step}$. The question is, what is this step? The step is the slope1. Ok, it seems remarkable. To find the minimum point, we can use slope1. Let's say slope1 $= 3$ and update our weight. $w := w - \\text{slope1} \\Rightarrow w = 2$.\n* Now, our weight $w$ is $2$. As you remember, we need to find the cost function with forward propagation again.\n* Let's say, according to forward propagation with $w = 2$, the cost function is $0.4$. Hmm, we are heading in the right direction because our cost function is decreasing. We have a new value for the cost function, which is $0.4$. Is that enough? Actually, I do not know; let's try one more step.\n* Slope2 $= 0.7$ and $w = 2$. Let's update the weight: $w := w - \\text{step}(\\text{slope2}) \\Rightarrow w = 1.3$, which is the new weight. So, let's find the new cost.\n* Perform one more forward propagation with $w = 1.3$, and our cost $= 0.3$. Ok, our cost even decreased. It looks fine, but is it enough, or do we need to take one more step? The answer is, again, I do not know; let's try.\n* Slope3 $= 0.01$ and $w = 1.3$. Updating weight: $w := w - \\text{step}(\\text{slope3}) \\Rightarrow w = 1.29 \\approx 1.3$. The weight does not change because we found the minimum point of the cost function.\nEverything seems good, but how do we find the slope? If you remember from high school or university, to find the slope of a function (cost function) at a given point (at a given weight), we take the derivative of the function at the given point. You may ask, how does it know where to go? You can say that it can go to higher cost values instead of going to the minimum point. The answer is that the slope (derivative) gives both the step and the direction of the step. Therefore, do not worry :)\n* The update equation is this. It says that there is a cost function (which takes weight and bias). Take the derivative of the cost function according to weight and bias. Then multiply it by $\\alpha$, the learning rate. Then update the weight. (In order to explain, I ignore bias, but all these steps will be applied to bias as well)\n\n<a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/hYTTJH/8.jpg\" alt=\"8\" border=\"0\"></a>\n\n\n* Now, I'm sure you are asking, what is the **learning rate** that I mentioned earlier? It is a very simple term that determines the learning rate. However, there is a tradeoff between learning fast and never learning. For example, you are in Paris (current cost) and want to go to Madrid (minimum cost). If your speed (learning rate) is small, you can go to Madrid very slowly, and it takes too long. On the other hand, if your speed (learning rate) is big, you can go very fast, but maybe you crash and never get to Madrid. Therefore, we need to choose our speed (learning rate) wisely.\n* The learning rate is also called a hyperparameter that needs to be chosen and tuned. I will explain it more in detail in artificial neural networks with other hyperparameters. For now, just assume the learning rate is $1$ for our previous example.\n* I think now you understand the logic behind **forward propagation (from weights and bias to cost)** and **backward propagation (from cost to weights and bias to update them)**. Also, you learned about gradient descent. Before implementing the code, you need to learn one more thing: how to take the derivative of the cost function according to weights and bias. It is not related to Python or coding; it is pure mathematics. There are two options: the first one is to google how to take the derivative of the log loss function, and the second one is even to google what the derivative of the log loss function is :) I choose the second one because I cannot explain math without talking.\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","metadata":{}},{"cell_type":"code","source":"# In backward propagation we will use y_head that we found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\n\n\ndef forward_backward_propagation(w,b,X_train,y_train):\n    \n    # forward propagation\n    z = w.T @ X_train + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/X_train.shape[1]      # for scaling\n    \n    # backward propagation\n    derivative_weight = (X_train @ ((y_head-y_train).T)) / X_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train) / X_train.shape[1]\n    \n    # Dictionary of weights and biases derivatives\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Up to this point, we have learned:\n\n* Initializing the parameters (implemented)\n* Finding the cost with forward propagation and cost function (implemented)\n* Updating (learning) parameters (weight and bias). Now let's implement it.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T14:37:06.193430Z","iopub.execute_input":"2024-03-30T14:37:06.194503Z","iopub.status.idle":"2024-03-30T14:37:06.202627Z","shell.execute_reply.started":"2024-03-30T14:37:06.194466Z","shell.execute_reply":"2024-03-30T14:37:06.201477Z"}}},{"cell_type":"code","source":"# Updating the parameters (learning) \n\ndef update(w, b, X_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n\n    # updating(learning) the parameters, number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,X_train,y_train)\n        cost_list.append(cost)\n        # let's update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)  # add the cost after each 10th iteration\n            index.append(i)          # iteration index\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # weights and bias final update\n    parameters = {\"weight\": w,\"bias\": b}\n    \n    # ploting the cost results after each 10th iteration\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, X_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Up to this point, we have learned our parameters. It means we have fitted the data.\n* In order to make predictions, we need parameters. Therefore, let's predict.\n* During the prediction step, we have `X_test` as an input, and we use it to make forward predictions.","metadata":{}},{"cell_type":"code","source":"# Prediction function\n\ndef predict(w, b, X_test) :\n    # X_test is now the input for forward propagation\n    z = sigmoid(w.T @ X_test +b)\n    y_pred = np.zeros((1,X_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than/equal to 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]) :\n        if z[0,i] <= 0.5 :\n            y_pred[0,i] == 0\n        else :\n            y_pred[0,i] == 1\n    return y_pred\n\n#predict(parameters[\"weight\"],parameters[\"bias\"],X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(1,X_test.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We made the prediction.\n* Now let's put them all together.","metadata":{}},{"cell_type":"code","source":"def logistic_regression(X_train, y_train, X_test, y_test, learning_rate ,  num_iterations):\n    # initialize=ation of the parameters :\n    dimension =  X_train.shape[0]  # that's 4096\n    w,b = initialize_weights_and_bias(dimension)\n    \n    # We don't change the learning rate\n    parameters, gradients, cost_list = update(w, b, X_train, y_train, learning_rate,num_iterations)\n    \n    # Predictions \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],X_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],X_train)\n    \n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(X_train, y_train, X_test, y_test,learning_rate = 0.01, num_iterations = 150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## Logistic Regression with Sklearn\n* In sklearn library, there is a logistic regression method that ease implementing logistic regression.\n* To understand each parameter of logistic regression in sklearn, you can read from there http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n* The accuracies are different from what we find. Because logistic regression method uses a lot of different features that we didn't use like different optimization parameters or regularization.\n* Let's make a conclusion for logistic regression and continue with artificial neural network.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state = 42,max_iter= 150)\nclf_train = logreg.fit(X_train.T, y_train.T)\nclf_test = logreg.fit(X_test.T, y_test.T)\n\nprint(\"test accuracy: {} \".format(clf_test.score(X_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(clf_train.score(X_train.T, y_train.T)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n## Summary and Questions in Minds\n\n<font color='purple'>\nWhat we did in this first part:\n\n* Initialized parameters: weight and bias\n* Performed forward propagation\n* Calculated the loss function\n* Derived the cost function\n* Implemented backward propagation (gradient descent)\n* Made predictions using the learned parameters: weight and bias\n* Utilized logistic regression with `sklearn`\n* We will construct an artificial neural network based on logistic regression.\n\nHOMEWORK: This is a good place to pause and practice. Your homework assignment is to create your own logistic regression method and classify two different sign language digits.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n# Artificial Neural Network (ANN)\n\nCalled a **deep neural network** or **deep learning**.\n\n**What is a neural network ?**\nIt basically involves taking logistic regression and repeating it at least 2 times.\n\n* In logistic regression, there are input and output layers. However, in a neural network, there is at least one hidden layer between the input and output layers.\n\n**What is deep, in order to say \"deep\", and how many layers do I need to have?**\n\"'Deep' is a relative term, it of course refers to the 'depth' of a network, meaning how many hidden layers it has. 'How deep is your swimming pool?' could be 12 feet or it might be 2 feet; nevertheless, it still has a depth--it has the quality of 'deepness'. 32 years ago, people used two or three hidden layers. That was the limit for the specialized hardware of the day. Just a few years ago, 20 layers was considered pretty deep. In October, Andrew Ng mentioned 152 layers was (one of) the biggest commercial networks he knew of. Last week, I talked to someone at a big, famous company who said he was using 'thousands'. So I prefer to just stick with 'How deep?'\"\n**Why is it called hidden ?**\nBecause the hidden layer does not see the inputs (training set).\nFor example, if you have input, one hidden, and output layers, when someone asks you \"hey, my friend, how many layers does your neural network have?\" The answer is \"I have a 2-layer neural network\". Because **while computing the layer number, the input layer is ignored**.\nLet's see a 2-layer neural network:\n<a href=\"http://ibb.co/eF315x\"><img src=\"http://preview.ibb.co/dajVyH/9.jpg\" alt=\"9\" border=\"0\"></a>\n\nStep by step, we will learn about this image:\n\n* As you can see, there is one hidden layer between the input and output layers, and this hidden layer has 3 nodes. If you're curious why I chose the number of nodes as 3, the answer is there is no reason, I just chose it that way :). The number of nodes is a hyperparameter like the learning rate. Therefore, we will discuss hyperparameters at the end of artificial neural networks.\n\n* Input and output layers do not change. They are the same as logistic regression.\n\n* In the image, there is a $tanh$ function, it is an activation function like the sigmoid function. **The $tanh$ activation function is better than sigmoid for hidden units because the mean of its output is closer to zero, so it centers the data better for the next layer**. Also, the $tanh$ activation function increases non-linearity, which helps our model learn better.\n\n* As you can see, with the purple color, there are two parts. Both parts are like logistic regression. The only difference is the activation function, inputs, and outputs.\n    * **In logistic regression: input => output**\n    * **In a 2-layer neural network: input => hidden layer => output**. You can think of the hidden layer as the output of part 1 and the input of part 2.\n    \n* That's all. We will follow the same path as logistic regression for the 2-layer neural network.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T18:17:52.828726Z","iopub.execute_input":"2024-03-30T18:17:52.829641Z","iopub.status.idle":"2024-03-30T18:17:52.837540Z","shell.execute_reply.started":"2024-03-30T18:17:52.829603Z","shell.execute_reply":"2024-03-30T18:17:52.836426Z"}}},{"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## 2-Layer Neural Network\n\n* 1/ Size of layers and initializing parameters weights and bias\n* 2/ Forward propagation\n* 3/ Loss function and Cost function\n* 4/ Backward propagation\n* 5/ Update Parameters\n* 6/ Prediction with learnt parameters weight and bias\n* 7/ Create Model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n## 2-1) Size of layers and initializing parameters weights and bias\n\n* For `X_train` that has 349 samples (349 images, 1 image per column) $x^{(349)}$:\n$$z^{[1] (349)} =  W^{[1]} x^{(349)} + b^{[1] (349)}$$ \n$$a^{[1] (349)} = \\tanh(z^{[1] (349)})$$\n$$z^{[2] (349)} = W^{[2]} a^{[1] (349)} + b^{[2] (349)}$$\n$$\\hat{y}^{(349)} = a^{[2] (349)} = \\sigma(z^{ [2] (349)})$$\n\n* In logistic regression, we initialized weights to 0.01 and bias to 0. However, now we initialize weights randomly, because if we initialize the parameters to zero, each neuron in the first hidden layer will perform the same computation. Therefore, even after multiple iterations of gradient descent, each neuron in the layer will compute the same thing as the other neurons. Thus, we initialize them randomly. Additionally, initial weights should be small. If they are very large initially, this will cause the inputs of the tanh function to be very large, resulting in gradients close to zero (vanishing gradient). Consequently, the optimization algorithm will be slow.\n\n* Bias can be initialized to zero initially.\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Initialization of the parameters and layer sizes\nlayer_size = 3\ndef initialize_paramters_and_layer_sizes_NN(X_train, y_train) :\n    parameters = {\"weight1\" : np.random.randn(layer_size, X_train.shape[0])* 0.1, #shape(3,4096)\n               \"bias1\" : np.zeros((layer_size,1)),\n               \"weight2\" : np.random.randn(y_train.shape[0],layer_size)*0.1,\n               \"bias2\" : np.zeros((y_train.shape[0], 1))}\n    return parameters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n## 2-2) Forward propagation\n* Forward propagation is almost the same as logistic regression.\n* The only difference is that we use the $tanh$ function and repeat the entire process twice.\n* Also, `NumPy` has a tanh function, so we do not need to implement it.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T18:49:04.361703Z","iopub.execute_input":"2024-03-30T18:49:04.362317Z","iopub.status.idle":"2024-03-30T18:49:04.371630Z","shell.execute_reply.started":"2024-03-30T18:49:04.362272Z","shell.execute_reply":"2024-03-30T18:49:04.369833Z"}}},{"cell_type":"code","source":"def forward_propagation_NN(X_train, parameters) :\n    # Z1 : (layer_size, X_train.shape[0])@ (layer_size, X_train.shape[0], layer_size, X_train.shape[1]) ---> shape(layer_size,X_train.shape[1])\n    Z1 = parameters[\"weight1\"]@X_train + parameters[\"bias1\"]\n    # A1 : shape(layer_size,X_train.shape[1])\n    A1 = np.tanh(Z1)\n    # Z2 : shape(y_train.shape[0],layer_size)@shape(layer_size,X_train.shape[1]) ---- > shape(y_train.shape[0],X_train.shape[1])\n    Z2 = parameters[\"weight2\"]@A1 + parameters['bias2']\n    # A2 : shape(y_train.shape[0],X_train.shape[1])\n    A2 = sigmoid(Z2)\n    \n    cache = {\"Z1\": Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n    \n    return A2, cache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n## 2-3) Loss function and Cost function\n* Loss and cost functions are same as with logistic regression\n* Cross entropy function\n<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/nyR9LU/as.jpg\" alt=\"as\" border=\"0\"></a><br />","metadata":{}},{"cell_type":"code","source":"# Compute the cost\ndef compute_cost_NN(A2, y, parameters):\n    \"\"\"A2 : logits\n    y : ground truths\n    parameters : the weights and biases\"\"\"\n    \n    logprobs = np.multiply(np.log(A2), y)\n    cost = -np.sum(logprobs)/y.shape[1]  # average\n    return cost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n## 2-4) Backward propagation\n* As you know backward propagation means derivative.\n\n* The logic is the same, let's write code.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:30:53.035129Z","iopub.execute_input":"2024-03-30T19:30:53.035549Z","iopub.status.idle":"2024-03-30T19:30:53.642640Z","shell.execute_reply.started":"2024-03-30T19:30:53.035516Z","shell.execute_reply":"2024-03-30T19:30:53.640911Z"}}},{"cell_type":"code","source":"# backward propagation\ndef backward_propagation_NN(parameters, cache, X, y) :\n    \"\"\"parameters : weights and biases\n    cache : dictionary of the inputs and outputs at each step\n    X : X_train\n    y : ground truths\"\"\"\n    # shape of dX is same as the shape of X :\n    # shape of dZ2 : (y_train.shape[0],X_train.shape[1])\n    dZ2 = cache[\"A2\"] - y\n    # dW2 : (y_train.shape[0],layer_size)\n    dW2 = (dZ2 @ cache[\"A1\"].T)/X.shape[1]\n    # db2 : (layer_size, 1)\n    db2 = np.sum(dZ2,axis =1,keepdims=True)/X.shape[1]\n    # dZ1 : (layer_size, y_train.shape[0])@(y_train.shape[0],X_train.shape[1]) --------> (layer_size,X_train.shape[1])\n    dZ1 = (parameters[\"weight2\"].T @ dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    # dW1 : (layer_size, X_train.shape[0])\n    dW1 = (dZ1@X.T)/X.shape[1]\n    # db1 : (layer_size, 1)\n    db1 = np.sum(dZ1,axis =1,keepdims=True)/X.shape[1]\n    \n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n## 2-5) Updatig the Parameters \n* Updating the parameters is also the same with logistic regression.\n* We actually do alot of work with logistic regression","metadata":{}},{"cell_type":"code","source":"# update the parameters\ndef update_parameters_NN(parameters, grads, learning_rate =1e-2) :\n    parameters = {\"weight1\" : parameters['weight1'] - learning_rate*grads[\"dweight1\"],\n                 \"bias1\" : parameters['bias1'] - learning_rate*grads[\"dbias1\"],\n                 \"weight2\" : parameters['weight2'] - learning_rate*grads[\"dweight2\"],\n                 \"bias2\" : parameters['bias2'] - learning_rate*grads[\"dbias2\"]}\n    return parameters\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n## 2-6) Prediction with the learned parameters weight and bias\n* Let's write a predict function like we did with logistic regression.","metadata":{}},{"cell_type":"code","source":"# Prediction\ndef predict_NN(parameters, X_test):\n    # Forward propagation\n    # X_test is the input for forward propagation\n    A2, cache = forward_propagation_NN(X_test, parameters)\n    y_pred = np.zeros((1, X_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than/equal to 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]) :\n        if A2[0,i] <= 0.5 :\n            y_pred[0,i] = 0\n        else :\n            y_pred[0,i] = 1\n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n## 2-7) Create Model\n* Let's put the codes all together.","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:53:54.382529Z","iopub.execute_input":"2024-03-30T19:53:54.383785Z","iopub.status.idle":"2024-03-30T19:53:54.393041Z","shell.execute_reply.started":"2024-03-30T19:53:54.383732Z","shell.execute_reply":"2024-03-30T19:53:54.391116Z"}}},{"cell_type":"code","source":"# 2-layer neural network\ndef two_layer_NN(X_train, y_train, X_test, y_test, num_iterations) :\n    cost_list = []\n    index_list = []\n    \n    # Parameters initialization and Layer Sizes\n    parameters = initialize_paramters_and_layer_sizes_NN(X_train, y_train)\n    \n    for i in range(num_iterations) :\n        # Forward propagation\n        A2, cache = forward_propagation_NN(X_train, parameters)\n        \n        # Compute the cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n        \n        # backward propagation\n        grads = backward_propagation_NN(parameters, cache, X_train, y_train)\n        \n        # Update the parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i%100 == 0:\n            cost_list.append(cost)  # list of costs after each 100 iterations\n            index_list.append(i)    # list of indexes\n            print(\"Cost after iteration %i : %f\" %(i, cost))  # %i for integer, %f for float\n    \n    # Plotting the cost \n    plt.plot(index_list, cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # Prediction\n    y_pred_test = predict_NN(parameters, X_test)\n    y_pred_train = predict_NN(parameters, X_train)\n    \n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n    return parameters\n    \nparameters = two_layer_NN(X_train, y_train, X_test, y_test, num_iterations = 2600)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='purple'>\nUp to this point we create 2-layer neural network and learned how to implement :\n* The size of layers and initializing the weights and bias parameters\n* Forward propagation\n* Loss function and Cost function\n* Backward propagation\n* Update Parameters\n* Prediction with learned parameters weight and bias\n* Create Model\n\n<br> Now lets learn how to implement L-layer neural network with keras.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"19\"></a> <br>\n# L Layer Neural Network\n\n* **What happens if the number of hidden layers increases ? Earlier layers can detect simple features.**\n\n* When the model composes simple features together in the later layers of the neural network, it can learn more and more complex functions. For example, let's look at our sign one.\n<a href=\"http://ibb.co/dNgDJH\"><img src=\"http://preview.ibb.co/mpD4Qx/10.jpg\" alt=\"10\" border=\"0\"></a>\n* the first hidden layer learns edges or basic shapes like lines. When the number of layers increases, layers start to learn more complex things like convex shapes or characteristic features like the forefinger.\n* Let's create our model:\n    * There are some hyperparameters we need to choose like learning rate, number of iterations, number of hidden layers, number of hidden units, and the type of activation functions.\n    \n    * These hyperparameters can be chosen intuitively if you spend a lot of time in the deep learning world. However, if you don't spend too much time, the best way is to Google it, but it is not necessary. You need to try hyperparameters to find the best one.\n    \n     * In this tutorial, our model will have **2 hidden layers with 8 and 4 nodes, respectively**. Because when the number of hidden layers and nodes increases, it takes too much time.\n     \n     * As an activation function, we will use **ReLU (Rectified Linear Unit) for the first hidden layer, ReLU for the second hidden layer, and sigmoid for the output layer, respectively**.\n     \n    * The **number of iterations will be 100**.\n    \n* Our approach is the same as in previous parts, however, as we learn the logic behind deep learning, we can ease our job and use the `Keras` for deeper neural networks.\n\n* First, let's reshape our `X_train`, `X_test`, `y_train`, and `y_test`.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"22\"></a> <br>\n## Implementing with keras library\nLets look at some parameters of keras library:\n* **units**: output dimensions of node\n* **kernel_initializer**: to initialize weights\n* **activation**: activation function, we use relu\n* **input_dim**: input dimension that is number of pixels in our images (4096 px)\n* **optimizer**: we use adam optimizer\n    * Adam is one of the most effective optimization algorithms for training neural networks.\n    * Some advantages of Adam is that relatively low memory requirements and usually works well even with little tuning of hyperparameters\n* **loss**: Cost function is the same. By the way the name of the cost function is **cross-entropy cost function** that we use previous parts.\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n* **metrics**: it's accuracy.\n* **cross_val_score**: we use cross validation (https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners)\n* **epochs**: number of iteration","metadata":{}},{"cell_type":"code","source":"!pip install scikeras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the ANN\n\nfrom scikeras.wrappers import KerasClassifier  # Import from SciKeras\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow import keras  # Import TensorFlow for Keras\nfrom tensorflow.keras.layers import Dense  # Import layers from Keras\nfrom keras.models import Sequential\n\n\n\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    # 1st layer : 8 neurons, Relu activation\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n    # 2nd layer : 4 neurons, Relu activation\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    # 3rd layer : 1 neuron, sigmoid activation\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    \n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}